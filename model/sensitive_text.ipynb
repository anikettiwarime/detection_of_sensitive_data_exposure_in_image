{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-z0Vwvod3Xd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX5KXUt7wg0j"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Downloading the dataset\n",
        "git_folder = \"/content/drive/MyDrive\"\n",
        "\n",
        "\n",
        "dataset_folder = git_folder + \"/text_dataset/\"\n",
        "sensitive_datafile = \"SensitiveDataset.json\"\n",
        "nonsensitive_datafile = \"NonSensitiveDataset.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDbRANDXjKCl"
      },
      "outputs": [],
      "source": [
        "# Necessary Variables\n",
        "#hyperparameters and settings used throughout the code for tokenization, padding, and model training.\n",
        "vocab_size = 3000\n",
        "embedding_dim = 32\n",
        "max_length = 70\n",
        "truncation_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "training_size = 20000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd1BuOmAjKJk",
        "outputId": "6d48e96d-193e-48af-d65b-53c06353af5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n",
            "\u001b[0mDataset Size:  31606\n",
            "Training Dataset Size:  20000\n",
            "Sample Training Data: In 1965, burning tree yard, rebel teenager Heather Fasulo (Agnes Bruckner) sent boarding school Falburn Academy middle woods estranged mother Alice Fasulo (Emma Campbell) negligent father Joe Fasulo (Bruce Campbell). The headmistress, Ms. Traverse (Patricia Clarkson), accepts Heather spite father's bad financial condition. The displaced Heather becomes close friends Marcy Turner (Lauren Birkell), maltreated abusive classmate Samantha Wise (Rachel Nichols). During night, Heather nightmare student named Ann, covered blood, hears voices seem coming woods. The next day, Marcy tells Heather Ann taken mental institution attempting commit suicide, covered blood.\n",
            "With help Marcy, Heather eventually learns adjust new school, even fun times making friends. Ms. Traverse subjects Heather special tests see \"gifted,\" telling part scholarship academy. The girls tell Heather spooky story history Falburn, includes three young redheaded sisters arrived school turned witches, killing headmistress leaving woods. Meanwhile, Samantha continues torment Heather, comes despise fights back. Ann returns mental institution, Heather finds one day, rocking bed. Ann reveals afraid will taken witches. She says cold, Heather climbs trunk try close open window Ann's bed. A low fog rushes room knocks Heather down, twisting ankle, taken infirmary. The next day, Heather finds Ann's bed empty, place filled dead leaves. She witnesses headmistress lying police Ann's disappearance, remarking taken care of.\n",
            "This leads become suspicious tries talk Marcy it. But Marcy acts strangely, shadowed one teachers. Soon after, Heather finds Marcy's bed empty covered leaves. Later, confronted woods Samantha, reveals actually trying protect Heather antics. She tells Heather school led coven witches want take girls away. Samantha explains called Heather's father help escape milk poisoned. The girls caught school mistress, promptly takes Samantha away. Samantha later found hanging long noose cafeteria. When police officer comes investigate, Heather tells missing students. The officer confronts headmistress, claims girls ran away. Another mistress \"leads\" officer woods find girls, killed living vines tree.\n",
            "Heather's parents show take home, though headmistress tries persuade otherwise. On way home, car mysteriously flipped Heather knocked unconscious. Alice dragged car living vine kicks Joe head, knocking out. Heather Joe wake nearby hospital. Before can reach other, Ms. Traverse Heather dragged away, slits hand forces black blood Joe's throat, puts catatonic state. Heather returns school despair. She drinks milk evening, later vomits back up, finding tree bark it. Back hospital, Joe wakes vomits Ms. Traverse's black blood, also tree bark it. He quickly escapes goes find Heather. That night, Heather begins hear voices again, attempts leave, living vine captures her.\n",
            "When awakens, wrapped vines large foggy room, next Ann Marcy, also held captive. All teachers appear reveal witches. Ms. Traverse leader, explains spirits trapped woods years, need inhabit bodies young women escape imprisonment. Heather appears centerpiece plan strongest powers among gifted students. Heather coerced completing ritual, vines begin mummify girls school. Before can complete itself, Joe breaks room ax begins kill witches. Heather breaks free vines grabs ax, proceeding chop witches pieces. Heather Joe leave girls, walking road daylight school burns distance behind them.\n",
            "The end movie states Falburn Academy burned ground 1965, surrounding woods strangely left untouched.\n",
            "Validation Dataset Size:  11606\n",
            "Sample Validation Data: number plate:tn 91 ki 4443\n"
          ]
        }
      ],
      "source": [
        "!pip install json\n",
        "import json\n",
        "dataList = []\n",
        "sentences = []\n",
        "labels = []\n",
        "# Stopwords should be removed or excluded from the given text so that more\n",
        "# focus can be given to those words which define the meaning of the text.\n",
        "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "\n",
        "def loadDataset(filename):\n",
        "  with open(dataset_folder + filename, 'r') as f:\n",
        "      datastore = json.load(f)\n",
        "  for item in datastore:\n",
        "    sentence = item['data']\n",
        "    label = item['is_sensitive']\n",
        "    for word in stopwords: #Remove stop words in sentence\n",
        "      token = \" \" + word + \" \"\n",
        "      sentence = sentence.replace(token, \" \")\n",
        "    dataList.append([sentence, label])\n",
        "\n",
        "# Loading both sensitive and non-sensitive dataset\n",
        "loadDataset(sensitive_datafile)\n",
        "loadDataset(nonsensitive_datafile)\n",
        "\n",
        "# Shuffling the dataset randomly\n",
        "random.shuffle(dataList)\n",
        "\n",
        "# Dataset size: 31500 (approx)\n",
        "print(\"Dataset Size: \", len(dataList))\n",
        "\n",
        "# Dataset has both sentences and labels\n",
        "for item in dataList:\n",
        "  sentences.append(item[0])\n",
        "  labels.append(item[1])\n",
        "\n",
        "# Splitting up the total dataset\n",
        "# Training size = 20000\n",
        "# Validation size = 11500 (approx)\n",
        "training_sentences = sentences[0:training_size]\n",
        "validation_sentences = sentences[training_size:]\n",
        "training_labels = labels[0:training_size]\n",
        "validation_labels = labels[training_size:]\n",
        "\n",
        "print(\"Training Dataset Size: \", len(training_sentences))\n",
        "print(\"Sample Training Data:\", training_sentences[0])\n",
        "print(\"Validation Dataset Size: \", len(validation_sentences))\n",
        "print(\"Sample Validation Data:\", validation_sentences[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0biJko5GjKMg",
        "outputId": "7977c0a0-3585-4c0b-8a73-90e58fe8857d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of word index: 103201\n",
            "Saving the word index as JSON in: /content/word_index.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Define the directory where to save the word_index.json file\n",
        "output_folder = \"/content/\"\n",
        "\n",
        "# Ensure that the output folder exists, create it if it doesn't\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Save the word index (Used for deploying in web application)\n",
        "\n",
        "\n",
        "# Tokenizer takes the num_words (here vocab_size = 3000) maximum occuring unique words from the dataset.\n",
        "# Anything out of these words will be treated as Out of Vocabulary(<oov>)\n",
        "# It strips the punctutations and removes upper-case letters.\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "\n",
        "# Apply the tokenizer on training sentences and generate the word index\n",
        "# Eg: word_index[\"the\"] = 1; word_index[\"cat\"] = 2; etc.\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "# Save the word index (Used for deploying in web application)\n",
        "word_index = tokenizer.word_index\n",
        "print(\"Size of word index:\", len(word_index))\n",
        "\n",
        "with open(os.path.join(output_folder, \"word_index.json\"), \"w\") as outfile:\n",
        "    json.dump(word_index, outfile)\n",
        "    print(\"Saving the word index as JSON in:\", os.path.join(output_folder, \"word_index.json\"))\n",
        "\n",
        "# with open(\"word_index.json\", \"w\") as outfile:\n",
        "#     json.dump(word_index, outfile)\n",
        "#     print(\"Saving the word index as JSON\")\n",
        "\n",
        "# Transforms each word in sentences to a sequence of integers based on the word_index\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "# To feed the text into neural network - sentences must be of the same length. Hence we'll be using padding.\n",
        "# If the sentences are smaller than the maxlen, then we'll pad (Here, we are using post padding)\n",
        "# If the sentences are larger than the maxlen, then we'll truncate (Here, we are using post truncation)\n",
        "\n",
        "#training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=truncation_type)\n",
        "training_padded = pad_sequences(training_sequences,padding=padding_type, truncating=truncation_type)\n",
        "\n",
        "# Apply the same for validation data\n",
        "validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\n",
        "\n",
        "validation_padded = pad_sequences(validation_sequences, padding=padding_type, truncating=truncation_type)\n",
        "validation_padded = pad_sequences(validation_sequences, padding=padding_type, truncating=truncation_type)\n",
        "#validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=truncation_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hytca1MNjKP1"
      },
      "outputs": [],
      "source": [
        "# Convert to Numpy arrays, so as to get it to work with TensorFlow 2.x\n",
        "import numpy as np\n",
        "training_padded = np.array(training_padded)\n",
        "training_labels = np.array(training_labels)\n",
        "validation_padded = np.array(validation_padded)\n",
        "validation_labels = np.array(validation_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBHqyg1rb5CE"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# # Get the current working directory\n",
        "# current_dir = os.getcwd()\n",
        "\n",
        "# # List the contents of the current directory\n",
        "# print(\"Current directory:\", current_dir)\n",
        "# print(\"Contents of the directory:\")\n",
        "# for item in os.listdir(current_dir):\n",
        "#     print(\"-\", item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynX0yYIbjKTN",
        "outputId": "335c8d01-7f88-4f9f-9e06-ac32b08354dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 32)          96000     \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, None, 64)          10304     \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 64)                0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 24)                1560      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 25        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 107889 (421.44 KB)\n",
            "Trainable params: 107889 (421.44 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Callbacks to cancel training after reaching a desired accuracy\n",
        "# This is done to avoid overfitting\n",
        "DESIRED_ACCURACY = 0.999\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if logs.get('accuracy') > DESIRED_ACCURACY:\n",
        "      print(\"Reached 99.9% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()\n",
        "\n",
        "# Sequential - defines a SEQUENCE of layers in the neural network.\n",
        "#The specific architecture chosen (embedding + convolutional + pooling + dense layers) is commonly used for text classification tasks and\n",
        "#has been shown to be effective in capturing relevant features from text data.\n",
        "model = tf.keras.Sequential([\n",
        "    # Embedding - Turns positive integers (indexes) into dense vectors of fixed size (here embedding_dim = 32).4\n",
        "    #An embedding layer converts word indices into dense vectors, capturing semantic meaning of words in a lower-dimensional space.\n",
        "   # tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "   tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    # 1D convolution layer - filter size = 128, convolution window = 5, activation fn = ReLU\n",
        "    #A convolutional layer extracts features from the embedded representations.\n",
        "    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
        "    # Global average pooling operation (Flattening)\n",
        "    #A global average pooling layer reduces dimensionality and summarizes the extracted features.\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    # Regular densely-connected Neural Network layer with ReLU activation function.\n",
        "    #Dense layers further process the features and make predictions.\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    # Regular densely-connected Neural Network layer with sigmoid activation function.\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# model.compile - Configures the model for training.\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "# Adam -  optimization algorithm used instead of the classical stochastic gradient descent procedure to update network weights.\n",
        "\n",
        "# Display the summary of the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q_6kCMzjKcB",
        "outputId": "a518e8b3-1588-45ce-910d-2555f17d95ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 113s 180ms/step - loss: 0.1307 - accuracy: 0.9528 - val_loss: 0.0417 - val_accuracy: 0.9833\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 91s 146ms/step - loss: 0.0161 - accuracy: 0.9962 - val_loss: 0.0183 - val_accuracy: 0.9925\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 87s 139ms/step - loss: 0.0090 - accuracy: 0.9981 - val_loss: 0.0132 - val_accuracy: 0.9941\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 92s 147ms/step - loss: 0.0052 - accuracy: 0.9988 - val_loss: 0.0061 - val_accuracy: 0.9984\n",
            "Epoch 5/10\n",
            "508/625 [=======================>......] - ETA: 13s - loss: 0.0030 - accuracy: 0.9991"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "\n",
        "# model.fit - Train the model for a fixed number of epochs\n",
        "history = model.fit(training_padded,\n",
        "                    training_labels,\n",
        "                    epochs=num_epochs,\n",
        "                    validation_data=(\n",
        "                        validation_padded,\n",
        "                        validation_labels),\n",
        "                    verbose=1)\n",
        "                    #callbacks=[callbacks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU6suu6wp0R9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the accuracy and loss functions\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6eO4gh1p0Uz"
      },
      "outputs": [],
      "source": [
        "import seaborn\n",
        "print('Confusion Matrix')\n",
        "y_predicted = model.predict(validation_padded)\n",
        "y_predicted_labels = y_predicted > 0.5\n",
        "\n",
        "size = np.size(y_predicted_labels)\n",
        "y_predicted_labels = y_predicted_labels.reshape(size, )\n",
        "\n",
        "for i in range (1, 5):\n",
        "  total = i * size // 4\n",
        "  cm = tf.math.confusion_matrix(labels=validation_labels[0:total],predictions=y_predicted_labels[0:total])\n",
        "\n",
        "  # Calculate accuracy\n",
        "  cm_np = cm.numpy()\n",
        "  conf_acc = (cm_np[0, 0] + cm_np[1, 1])/ np.sum(cm_np) * 100\n",
        "  print(\"Accuracy for\", str(total), \"Test Data = \", conf_acc)\n",
        "\n",
        "  # Plot the confusion matrix\n",
        "  plt.figure(figsize = (10,7))\n",
        "  seaborn.heatmap(cm, annot=True, fmt='d')\n",
        "  plt.title(\"Confusion Matrix for \" + str(total) + \" Test Data\")\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.ylabel('Expected')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mj6-mn4Lp_TB"
      },
      "outputs": [],
      "source": [
        "# Save and convert the model (Used for deploying in web application)\n",
        "model.save('model/text_model.h5', save_format='h5', include_optimizer=False)\n",
        "print(\"Saved the model successfully\")\n",
        "\n",
        "!apt-get -qq install virtualenv\n",
        "!virtualenv -p python3 venv\n",
        "!source venv/bin/activate\n",
        "!pip install -q tensorflowjs\n",
        "!tensorflowjs_converter --input_format=keras /content/model/text_model.h5\n",
        "\n",
        "\n",
        "# output_folder = \"/content/\"\n",
        "# Ensure that the output folder exists, create it if it doesn't\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "print(\"Model converted to JSON successfully\")\n",
        "model_config = model.to_json()\n",
        "model_config = json.loads(model_config)\n",
        "model_config.pop('config')['layers'][0]['config'].pop('batch_input_shape')\n",
        "with open(os.path.join(output_folder, \"model_config.json\"), \"w\") as outfile:\n",
        "    json.dump(model_config, outfile)\n",
        "    print(\"Saving the model config as JSON in:\", os.path.join(output_folder, \"model_config.json\"))\n",
        "# with open('model_config.json', 'w') as f:\n",
        "#     f.write(model_config)\n",
        "\n",
        "import tensorflow as tf\n",
        "# Define a custom function to load the model without the input_length parameter\n",
        "def load_model_without_input_length(filepath):\n",
        "    # Load the model with custom objects (if any)\n",
        "    model = tf.keras.models.load_model('model/text_model.h5', custom_objects=None, compile=True)\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "model = load_model_without_input_length('text_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sXPkneeyRqu"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install pytesseract\n",
        "!sudo apt install tesseract-ocr\n",
        "!sudo apt-get install tesseract-ocr-all\n",
        "\n",
        "import pytesseract\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# Function to perform OCR on the uploaded image\n",
        "def perform_ocr(image, lang):\n",
        "    # Convert image to grayscale\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Perform OCR using Tesseract\n",
        "    extracted_text = pytesseract.image_to_string(gray_image, lang=lang)\n",
        "\n",
        "    return extracted_text\n",
        "\n",
        "# Function to handle file upload\n",
        "def handle_file_upload(file_content):\n",
        "    image = Image.open(io.BytesIO(file_content))\n",
        "    image_array = np.array(image)\n",
        "    extracted_text = perform_ocr(image_array, lang='eng+mar+hin+kan')  # English, Marathi, Hindi, Kannada\n",
        "    return extracted_text\n",
        "\n",
        "# User interface for uploading image\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Perform OCR on the uploaded image and display the extracted text\n",
        "if len(uploaded) > 0:\n",
        "    for file_name, file_content in uploaded.items():\n",
        "        if file_name.endswith(('.jpg', '.jpeg', '.png')):\n",
        "            extracted_text = handle_file_upload(file_content)\n",
        "            print(\"Extracted Text:\")\n",
        "            print(extracted_text)\n",
        "        else:\n",
        "            print(\"Invalid file format. Please upload an image with .jpg, .jpeg, or .png extension.\")\n",
        "else:\n",
        "    print(\"No file uploaded.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEPPLUQgp_Yz"
      },
      "outputs": [],
      "source": [
        "# Sample examples\n",
        "#sentence = [\"phone no-91 24843899\", \"आधार - आम आदमी का अधिकार 0000 2222 8945\"]\n",
        "sentence = [extracted_text]\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=truncation_type)\n",
        "predictions = model.predict(padded)\n",
        "for i in range(len(predictions)):\n",
        "  print(predictions[i][0])\n",
        "  if predictions[i][0]>0.5:\n",
        "    print(\"Sensitive - \"+ sentence[i])\n",
        "  else:\n",
        "    print(\"Non-Sensitive - \" + sentence[i] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbAHfxO3wex0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYM8S5biikAR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}